{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95d74624",
   "metadata": {},
   "source": [
    "# Generation of job descriptions with tranformer models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3bb751",
   "metadata": {},
   "source": [
    "### Checking for GPU availability to run the computations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34dd86da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/.cache/pypoetry/virtualenvs/nlp-linkedin-offers-VrYHBMh4-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-05 20:22:05.983578: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-05 20:22:06.181716: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746469326.251890    2996 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746469326.271698    2996 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746469326.434199    2996 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746469326.434222    2996 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746469326.434224    2996 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746469326.434225    2996 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-05 20:22:06.451747: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc3cbbf",
   "metadata": {},
   "source": [
    "### Setting root path to project path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ded5c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = os.path.abspath(\n",
    "    os.path.join(os.getcwd(), '../../..')\n",
    ")\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e108844",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f2ca506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n",
      "Device count: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1d0f11",
   "metadata": {},
   "source": [
    "### Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e5f5b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: gpt2\n",
      "Tokenizer pad token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"  # Or choose another model like \"gpt2\", \"t5-small\", etc.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Set pad token if it doesn't exist (common for GPT-2 models)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "print(f\"Loaded model: {model_name}\")\n",
    "print(f\"Tokenizer pad token: {tokenizer.pad_token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754baadb",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "971d3483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded DataFrame from /home/gabriel/dev/SCIA/NLP_Linkedin_offers/data/processed/cleaned_postings_modeling.parquet. Shape: (122124, 4)\n",
      "                company_name  \\\n",
      "0      Corcoran Sawyer Smith   \n",
      "1     The National Exemplar    \n",
      "2     Abrams Fensterman, LLP   \n",
      "3  Downtown Raleigh Alliance   \n",
      "4                 Raw Cereal   \n",
      "\n",
      "                                               title  \\\n",
      "0                              Marketing Coordinator   \n",
      "1                        Assitant Restaurant Manager   \n",
      "2  Senior Elder Law / Trusts and Estates Associat...   \n",
      "3           Economic Development and Planning Intern   \n",
      "4                                           Producer   \n",
      "\n",
      "                                         description           location  \n",
      "0  Job description A leading real estate firm in ...      Princeton, NJ  \n",
      "1  The National Exemplar is accepting application...     Cincinnati, OH  \n",
      "2  Senior Associate Attorney Elder Law Trusts and...  New Hyde Park, NY  \n",
      "3  Job summary The Economic Development Planning ...        Raleigh, NC  \n",
      "4  Company Description Raw Cereal is a creative d...      United States  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 122124 entries, 0 to 122123\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   company_name  122124 non-null  object\n",
      " 1   title         122124 non-null  object\n",
      " 2   description   122124 non-null  object\n",
      " 3   location      122124 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 3.7+ MB\n",
      "None\n",
      "Filtered DataFrame. New shape: (122055, 4)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 122055 entries, 0 to 122123\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   company_name  122055 non-null  object\n",
      " 1   title         122055 non-null  object\n",
      " 2   description   122055 non-null  object\n",
      " 3   location      122055 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 4.7+ MB\n",
      "None\n",
      "\n",
      "Converted DataFrame to Dataset. Size: 122055\n",
      "\n",
      "Split dataset into training and testing sets:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['company_name', 'title', 'description', 'location', '__index_level_0__'],\n",
      "        num_rows: 109849\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['company_name', 'title', 'description', 'location', '__index_level_0__'],\n",
      "        num_rows: 12206\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 109849/109849 [00:33<00:00, 3310.02 examples/s]\n",
      "Map: 100%|██████████| 12206/12206 [00:03<00:00, 3536.18 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 109849\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 12206\n",
      "    })\n",
      "})\n",
      "\n",
      "Tokenized training dataset sample:\n",
      "{'input_ids': tensor([16844, 10027,  7913,  1596,  2931,  4761, 15768, 12489,   327,   600,\n",
      "          292,   318,  6095,   281,  2445,  2142,  3862,   284,  8277,   287,\n",
      "          257,  1430,  3562,   284,  2148,  9815,  3725,  1973,   477,  3006,\n",
      "          286,   674,  1597,    11,   355,   880,   355,   262,  4708,  2594,\n",
      "         2831,    13,   383,  3061,   318,   284, 15651,   262,  2445,   284,\n",
      "          674,  1180, 13346,    13,   383,  2445,  1998, 21001,  2832,   319,\n",
      "         3047,   351,   262,  3663,   284,  9427,   351,  3294, 13346,   287,\n",
      "         1502,   284,  1205,  7387,  5531,    11,  4009,    11,  6946,   290,\n",
      "          640,  4542,  4678,    13, 20389,  9537,  6637, 20906,  3334,  3961,\n",
      "         6031, 35728,   393,   402,  1961, 21327, 15461,   257, 33399,   264,\n",
      "         4922, 31278,  3819,   763,  1034,   393, 42329,  1998,   287,   257,\n",
      "        22092,  4809,    11, 17329,    11, 19174,   393, 30048,  2597,   317,\n",
      "         1597,   393,  4306,  3519,  8233,  1688,  4415, 19777,   351,  5413,\n",
      "         4452,  9678,    11, 24134,    11,  4935,    11, 30096,   290,   493,\n",
      "         2596,   316,  5230,   317,  5531,  4542,  2597,   287,  7611,   393,\n",
      "         3519,  1070, 11510,   333, 41001,  4568, 43138,   284,   923,  1626,\n",
      "          734,  2745,   706,  2897,   925,  6292,   327,   600,   292,  4394,\n",
      "         9815,   290,  7606,  3315,    11, 22727,   290,  5761,  4034,    11,\n",
      "         9593,  6538,  1535,  1337, 21884,   326,   389,  1542,  2793,   621,\n",
      "          262,  2260,  2811,   329,   674,  2831,    13,  1881,   286,   674,\n",
      "         3315,  1410,  3689,   318,   772,  4438,   379,  6632,  1575,   284,\n",
      "          674,  4887,    13, 12032,    11,  3954, 36824,  4887, 18179, 47665,\n",
      "         7119, 22219,   479, 42886, 36644, 36824, 10500, 33147,  1056,  6118,\n",
      "        46454,   290,  5155, 17541,  6400,  1095, 47355,  3862,  3242,   290,\n",
      "         6479, 13842, 32619, 25170,   434, 24922,   871,  7123, 42508,   287,\n",
      "        16137,    11,   327,   600,   292,   318,   257,  7271,  2714,  1664,\n",
      "        14018,   625,   262, 22767, 48539,  8060,  9683,  5991,   739,   262,\n",
      "         6194, 16356,  1921,   290,   318,   257,  7515,   286,  1111,   262,\n",
      "         8997, 23676,   264,  5323, 12901,   290, 22767, 48539,  1802, 12901,\n",
      "           13,   327,   600,   292, 10501,  5419,   517,   621, 15897,    11,\n",
      "          830,  5692,   286,   477,  3858,   290, 10620,   651, 20832,    56,\n",
      "          284,  1280,   511,  8215,   351,  6628,   790,  1110,   416,  4955,\n",
      "          257,  3094,  2837,   286,  3186,   290,  2594,   326,  9494,   674,\n",
      "         4297,  2939,   290,  1037,  1394,   511,  7291,   290,  4409,  3424,\n",
      "           11,  3338,   290,  2045,   511,  1266,    13,  2080,  3186,   290,\n",
      "         2594,  1390, 22551,    11, 46054,    11,   285,  2840,    11, 40018,\n",
      "         9416,    11,   717,  6133,   290,  3747,  3186,    11,  2046, 24995,\n",
      "        39116,   290,  4856,    11,   290,  3747,   290, 11846,  3047,    11,\n",
      "          327,   600,   292,  5419,  4297,   651, 23432,   329,   262,  5521,\n",
      "          820,   764,  1675,  1104,   674,  3349,  1973,  2258,  2253,    11,\n",
      "          356,   302,  6095,  7986, 11153,   351, 20505,   284,  1445,   510,\n",
      "         1626,   674,  1664,    13,  3954,  4708,  3968,    11,   674, 22445,\n",
      "          284,   674,  6538,  4887,   290, 44176,  3451,  6443,   777,   389,\n",
      "          655,   257,  1178,  4034,   356,   302,  6613,   284,  2897,    13,\n",
      "         3954,  6538,  4887,   760,   790,  1693,   318,  4688,    11,   290,\n",
      "          326, 48424, 10182, 11044,    13,  3914,   264,  1561,   546,   703,\n",
      "          345, 32660,  4197,   656,   674,  1074,   290,   703,   534,  1327,\n",
      "          670,   481,   307,  8018,   832,  7606,  1414,    11,   995,  1398,\n",
      "         4034,   290,  7044,  3451,  2478,    13,  4231,   345, 23432,   329,\n",
      "        44176,  6443,   379,   327,   600,   292,    30,   327,   600,   292,\n",
      "        10501,   318,   281,   412,  4720,  6708,  2533,   876,  7561, 12645,\n",
      "          263,   290]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1]), 'labels': tensor([16844, 10027,  7913,  1596,  2931,  4761, 15768, 12489,   327,   600,\n",
      "          292,   318,  6095,   281,  2445,  2142,  3862,   284,  8277,   287,\n",
      "          257,  1430,  3562,   284,  2148,  9815,  3725,  1973,   477,  3006,\n",
      "          286,   674,  1597,    11,   355,   880,   355,   262,  4708,  2594,\n",
      "         2831,    13,   383,  3061,   318,   284, 15651,   262,  2445,   284,\n",
      "          674,  1180, 13346,    13,   383,  2445,  1998, 21001,  2832,   319,\n",
      "         3047,   351,   262,  3663,   284,  9427,   351,  3294, 13346,   287,\n",
      "         1502,   284,  1205,  7387,  5531,    11,  4009,    11,  6946,   290,\n",
      "          640,  4542,  4678,    13, 20389,  9537,  6637, 20906,  3334,  3961,\n",
      "         6031, 35728,   393,   402,  1961, 21327, 15461,   257, 33399,   264,\n",
      "         4922, 31278,  3819,   763,  1034,   393, 42329,  1998,   287,   257,\n",
      "        22092,  4809,    11, 17329,    11, 19174,   393, 30048,  2597,   317,\n",
      "         1597,   393,  4306,  3519,  8233,  1688,  4415, 19777,   351,  5413,\n",
      "         4452,  9678,    11, 24134,    11,  4935,    11, 30096,   290,   493,\n",
      "         2596,   316,  5230,   317,  5531,  4542,  2597,   287,  7611,   393,\n",
      "         3519,  1070, 11510,   333, 41001,  4568, 43138,   284,   923,  1626,\n",
      "          734,  2745,   706,  2897,   925,  6292,   327,   600,   292,  4394,\n",
      "         9815,   290,  7606,  3315,    11, 22727,   290,  5761,  4034,    11,\n",
      "         9593,  6538,  1535,  1337, 21884,   326,   389,  1542,  2793,   621,\n",
      "          262,  2260,  2811,   329,   674,  2831,    13,  1881,   286,   674,\n",
      "         3315,  1410,  3689,   318,   772,  4438,   379,  6632,  1575,   284,\n",
      "          674,  4887,    13, 12032,    11,  3954, 36824,  4887, 18179, 47665,\n",
      "         7119, 22219,   479, 42886, 36644, 36824, 10500, 33147,  1056,  6118,\n",
      "        46454,   290,  5155, 17541,  6400,  1095, 47355,  3862,  3242,   290,\n",
      "         6479, 13842, 32619, 25170,   434, 24922,   871,  7123, 42508,   287,\n",
      "        16137,    11,   327,   600,   292,   318,   257,  7271,  2714,  1664,\n",
      "        14018,   625,   262, 22767, 48539,  8060,  9683,  5991,   739,   262,\n",
      "         6194, 16356,  1921,   290,   318,   257,  7515,   286,  1111,   262,\n",
      "         8997, 23676,   264,  5323, 12901,   290, 22767, 48539,  1802, 12901,\n",
      "           13,   327,   600,   292, 10501,  5419,   517,   621, 15897,    11,\n",
      "          830,  5692,   286,   477,  3858,   290, 10620,   651, 20832,    56,\n",
      "          284,  1280,   511,  8215,   351,  6628,   790,  1110,   416,  4955,\n",
      "          257,  3094,  2837,   286,  3186,   290,  2594,   326,  9494,   674,\n",
      "         4297,  2939,   290,  1037,  1394,   511,  7291,   290,  4409,  3424,\n",
      "           11,  3338,   290,  2045,   511,  1266,    13,  2080,  3186,   290,\n",
      "         2594,  1390, 22551,    11, 46054,    11,   285,  2840,    11, 40018,\n",
      "         9416,    11,   717,  6133,   290,  3747,  3186,    11,  2046, 24995,\n",
      "        39116,   290,  4856,    11,   290,  3747,   290, 11846,  3047,    11,\n",
      "          327,   600,   292,  5419,  4297,   651, 23432,   329,   262,  5521,\n",
      "          820,   764,  1675,  1104,   674,  3349,  1973,  2258,  2253,    11,\n",
      "          356,   302,  6095,  7986, 11153,   351, 20505,   284,  1445,   510,\n",
      "         1626,   674,  1664,    13,  3954,  4708,  3968,    11,   674, 22445,\n",
      "          284,   674,  6538,  4887,   290, 44176,  3451,  6443,   777,   389,\n",
      "          655,   257,  1178,  4034,   356,   302,  6613,   284,  2897,    13,\n",
      "         3954,  6538,  4887,   760,   790,  1693,   318,  4688,    11,   290,\n",
      "          326, 48424, 10182, 11044,    13,  3914,   264,  1561,   546,   703,\n",
      "          345, 32660,  4197,   656,   674,  1074,   290,   703,   534,  1327,\n",
      "          670,   481,   307,  8018,   832,  7606,  1414,    11,   995,  1398,\n",
      "         4034,   290,  7044,  3451,  2478,    13,  4231,   345, 23432,   329,\n",
      "        44176,  6443,   379,   327,   600,   292,    30,   327,   600,   292,\n",
      "        10501,   318,   281,   412,  4720,  6708,  2533,   876,  7561, 12645,\n",
      "          263,   290])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "parquet_file_path = os.path.join(project_root, \"data\", \"processed\", \"cleaned_postings_modeling.parquet\")\n",
    "text_column = \"description\" # Your column name\n",
    "block_size = 512 # Max sequence length for the model\n",
    "test_size = 0.1 \n",
    "random_seed = 42\n",
    "\n",
    "# --- Load Dataframe from Parquet ---\n",
    "try:\n",
    "    df = pd.read_parquet(parquet_file_path)\n",
    "    print(f\"Loaded DataFrame from {parquet_file_path}. Shape: {df.shape}\")\n",
    "    print(df.head()) # Optional\n",
    "    print(df.info()) # Optional\n",
    "    # filter out any rows with a description shorter than 100 characters\n",
    "    df = df[df[text_column].str.len() > 100]\n",
    "    print(f\"Filtered DataFrame. New shape: {df.shape}\")\n",
    "    print (df.info())\n",
    "    if text_column not in df.columns:\n",
    "        raise ValueError(f\"Column '{text_column}' not found in the DataFrame.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Parquet file not found at {parquet_file_path}\")\n",
    "    df = None\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Parquet file: {e}\")\n",
    "    df = None\n",
    "\n",
    "# --- Convert Pandas DataFrame to Hugging Face Dataset ---\n",
    "if df is not None:\n",
    "    # Convert the DataFrame to a single Dataset object first\n",
    "    full_dataset = Dataset.from_pandas(df)\n",
    "    print(f\"\\nConverted DataFrame to Dataset. Size: {len(full_dataset)}\")\n",
    "\n",
    "    # --- Split the Dataset ---\n",
    "    # Use train_test_split on the Dataset object\n",
    "    split_datasets = full_dataset.train_test_split(test_size=test_size, seed=random_seed)\n",
    "\n",
    "    # Rename the default 'test' split to 'validation' if preferred for Trainer, or keep as 'test'\n",
    "    # Trainer uses 'eval_dataset', so 'validation' or 'test' are common keys. Let's use 'test'.\n",
    "    # split_datasets['validation'] = split_datasets.pop('test') # Optional rename\n",
    "\n",
    "    print(\"\\nSplit dataset into training and testing sets:\")\n",
    "    print(split_datasets)\n",
    "\n",
    "    # Assign to raw_datasets (which is now a DatasetDict with 'train' and 'test')\n",
    "    raw_datasets = split_datasets\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping dataset conversion and splitting due to loading error.\")\n",
    "    raw_datasets = None\n",
    "\n",
    "# --- Tokenization Function (remains the same) ---\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the text\n",
    "    tokenized_output = tokenizer(examples[text_column], truncation=True, padding=\"max_length\", max_length=block_size)\n",
    "    # For Causal LM, labels are usually the same as inputs\n",
    "    tokenized_output[\"labels\"] = tokenized_output[\"input_ids\"].copy()\n",
    "    return tokenized_output\n",
    "\n",
    "# --- Apply Tokenization ---\n",
    "if raw_datasets:\n",
    "    # Apply tokenization to both splits ('train' and 'test')\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=raw_datasets[\"train\"].column_names # Remove original columns from both splits\n",
    "    )\n",
    "    # Set format for PyTorch\n",
    "    tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "    print(\"\\nTokenized dataset structure:\")\n",
    "    print(tokenized_datasets)\n",
    "\n",
    "    print(\"\\nTokenized training dataset sample:\")\n",
    "    if len(tokenized_datasets[\"train\"]) > 0:\n",
    "         print(tokenized_datasets[\"train\"][0])\n",
    "    else:\n",
    "        print(\"Tokenized training dataset is empty.\")\n",
    "\n",
    "    # Assign the splits\n",
    "    train_dataset = tokenized_datasets[\"train\"]\n",
    "    eval_dataset = tokenized_datasets[\"test\"] # Use the 'test' split for evaluation\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping tokenization due to dataset loading/conversion error.\")\n",
    "    train_dataset = None\n",
    "    eval_dataset = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c10c90",
   "metadata": {},
   "source": [
    "### Setup for the training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e00aaa2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments and Trainer initialized.\n",
      "Evaluation dataset size: 12206\n",
      "FP16 enabled: True\n",
      "Evaluation strategy: IntervalStrategy.STEPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2996/575851290.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "output_directory = os.path.join(project_root, \"models\", \"src\", \"generation\")\n",
    "use_fp16 = torch.cuda.is_available() # Enable FP16 only if GPU is available\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    torch_compile=True,\n",
    "    torch_compile_backend=\"inductor\",\n",
    "    torch_compile_mode=\"default\", \n",
    "    torch_empty_cache_steps=4,\n",
    "    output_dir=output_directory,\n",
    "    num_train_epochs=1,  # Start with 1 epoch for testing\n",
    "    per_device_train_batch_size=2,  # Adjust based on GPU memory\n",
    "    per_device_eval_batch_size=8, # Batch size for evaluation (can often be larger)\n",
    "    gradient_accumulation_steps=4, # Increase effective batch size\n",
    "    learning_rate=5e-5,\n",
    "    fp16=use_fp16, # Enable mixed precision training if GPU available\n",
    "    logging_dir=f\"{output_directory}/logs\",\n",
    "    logging_strategy=\"steps\", # Log metrics periodically\n",
    "    logging_steps=100,        # Log every 100 steps\n",
    "    eval_strategy=\"steps\", # Evaluate periodically\n",
    "    eval_steps=500,              # Evaluate every 500 steps\n",
    "    save_strategy=\"steps\",       # Save checkpoints periodically\n",
    "    save_steps=500,              # Save every 500 steps\n",
    "    load_best_model_at_end=True, # Load the best model found during evaluation at the end\n",
    "    metric_for_best_model=\"loss\", # Use evaluation loss to determine the best model (lower is better)\n",
    "    greater_is_better=False,     # Lower loss is better\n",
    "    save_total_limit=2,          # Keep only the last 2 checkpoints + the best one\n",
    "    report_to=\"none\",          # Disable external reporting (like wandb) for now\n",
    "    weight_decay=0.01,         # Regularization\n",
    "    dataloader_pin_memory=True, # Pin memory for faster data transfer to GPU\n",
    "    dataloader_num_workers=12, # Number of workers for data loading, leverage multiple CPU cores for faster data loading to GPU\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset if train_dataset else None,\n",
    "    eval_dataset=eval_dataset if eval_dataset else None, # Pass the evaluation dataset\n",
    "    tokenizer=tokenizer,\n",
    "    # data_collator=default_data_collator # Usually not needed for CausalLM unless custom padding\n",
    ")\n",
    "\n",
    "print(\"TrainingArguments and Trainer initialized.\")\n",
    "if eval_dataset:\n",
    "    print(f\"Evaluation dataset size: {len(eval_dataset)}\")\n",
    "print(f\"FP16 enabled: {use_fp16}\")\n",
    "print(f\"Evaluation strategy: {training_args.eval_strategy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b443f1b9",
   "metadata": {},
   "source": [
    "### Fine-tuning the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0b46457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13731' max='13731' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13731/13731 2:23:42, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.811600</td>\n",
       "      <td>2.785043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.767200</td>\n",
       "      <td>2.700827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.773600</td>\n",
       "      <td>2.640040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.695800</td>\n",
       "      <td>2.593958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.724100</td>\n",
       "      <td>2.553697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.628300</td>\n",
       "      <td>2.523878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.636700</td>\n",
       "      <td>2.500995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.563100</td>\n",
       "      <td>2.471948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.636000</td>\n",
       "      <td>2.451429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.541600</td>\n",
       "      <td>2.433481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.533700</td>\n",
       "      <td>2.414849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.496700</td>\n",
       "      <td>2.409583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.508600</td>\n",
       "      <td>2.386384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.510100</td>\n",
       "      <td>2.378803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.475200</td>\n",
       "      <td>2.372317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.437500</td>\n",
       "      <td>2.355689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>2.445200</td>\n",
       "      <td>2.348116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.525700</td>\n",
       "      <td>2.337905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>2.463300</td>\n",
       "      <td>2.329168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.434500</td>\n",
       "      <td>2.324465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>2.393000</td>\n",
       "      <td>2.320230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.369800</td>\n",
       "      <td>2.316181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>2.421900</td>\n",
       "      <td>2.311606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.391400</td>\n",
       "      <td>2.305984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>2.380700</td>\n",
       "      <td>2.304607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>2.388800</td>\n",
       "      <td>2.301676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>2.377900</td>\n",
       "      <td>2.302065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "Train Output Metrics: {'train_runtime': 8626.2415, 'train_samples_per_second': 12.734, 'train_steps_per_second': 1.592, 'total_flos': 2.8702407131136e+16, 'train_loss': 2.5315315686087607, 'epoch': 0.9999817933545744}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  total_flos               = 26731199GF\n",
      "  train_loss               =     2.5315\n",
      "  train_runtime            = 2:23:46.24\n",
      "  train_samples_per_second =     12.734\n",
      "  train_steps_per_second   =      1.592\n"
     ]
    }
   ],
   "source": [
    "if train_dataset:\n",
    "    print(\"Starting training...\")\n",
    "    try:\n",
    "        train_result = trainer.train()\n",
    "        print(\"Training finished.\")\n",
    "        # You can print some metrics from train_result if needed\n",
    "        metrics = train_result.metrics\n",
    "        print(f\"Train Output Metrics: {metrics}\")\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during training: {e}\")\n",
    "else:\n",
    "    print(\"Skipping training because the dataset was not loaded properly.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9bceb1",
   "metadata": {},
   "source": [
    "### Save the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ded73d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving final model and tokenizer to /home/gabriel/dev/SCIA/NLP_Linkedin_offers/models/src/generation/final...\n",
      "Model and tokenizer saved successfully.\n"
     ]
    }
   ],
   "source": [
    "final_model_path = f\"{output_directory}/final\"\n",
    "\n",
    "if train_dataset: # Only save if training actually happened\n",
    "    print(f\"Saving final model and tokenizer to {final_model_path}...\")\n",
    "    try:\n",
    "        trainer.save_model(final_model_path)\n",
    "        tokenizer.save_pretrained(final_model_path)\n",
    "        print(\"Model and tokenizer saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model/tokenizer: {e}\")\n",
    "else:\n",
    "    print(\"Skipping final model saving as training did not run.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-linkedin-offers-VrYHBMh4-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
