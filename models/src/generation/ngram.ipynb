{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a49c3839",
   "metadata": {},
   "source": [
    "# N-Gram model for job description generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "146358a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../../..')) # Adjust '..' if your notebook is deeper\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb7f8ed",
   "metadata": {},
   "source": [
    "### Download necessary resources for nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04bb7603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK 'punkt' tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Gabriel/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/Gabriel/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK 'punkt_tab'...\n",
      "NLTK resources checked/downloaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK 'punkt' tokenizer...\")\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK 'stopwords'...\")\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "# Add this check and download for punkt_tab\n",
    "try:\n",
    "    # Check for the specific English directory within punkt_tab\n",
    "    nltk.data.find('tokenizers/punkt_tab/english/') \n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK 'punkt_tab'...\")\n",
    "    nltk.download('punkt_tab')\n",
    "\n",
    "print(\"NLTK resources checked/downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "810ba72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_name</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Corcoran Sawyer Smith</td>\n",
       "      <td>Marketing Coordinator</td>\n",
       "      <td>job descriptiona leading real estate firm in n...</td>\n",
       "      <td>Princeton, NJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The National Exemplar</td>\n",
       "      <td>Assitant Restaurant Manager</td>\n",
       "      <td>the national exemplar is accepting application...</td>\n",
       "      <td>Cincinnati, OH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abrams Fensterman, LLP</td>\n",
       "      <td>Senior Elder Law / Trusts and Estates Associat...</td>\n",
       "      <td>senior associate attorney elder law trusts and...</td>\n",
       "      <td>New Hyde Park, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Downtown Raleigh Alliance</td>\n",
       "      <td>Economic Development and Planning Intern</td>\n",
       "      <td>job summarythe economic development planning i...</td>\n",
       "      <td>Raleigh, NC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Raw Cereal</td>\n",
       "      <td>Producer</td>\n",
       "      <td>company descriptionraw cereal is a creative de...</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122119</th>\n",
       "      <td>Lozano Smith</td>\n",
       "      <td>Title IX/Investigations Attorney</td>\n",
       "      <td>our walnut creek office is currently seeking a...</td>\n",
       "      <td>Walnut Creek, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122120</th>\n",
       "      <td>Pinterest</td>\n",
       "      <td>Staff Software Engineer, ML Serving Platform</td>\n",
       "      <td>about pinterest millions of people across the ...</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122121</th>\n",
       "      <td>EPS Learning</td>\n",
       "      <td>Account Executive, Oregon/Washington</td>\n",
       "      <td>company overview eps learning is a leading k12...</td>\n",
       "      <td>Spokane, WA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122122</th>\n",
       "      <td>Trelleborg Applied Technologies</td>\n",
       "      <td>Business Development Manager</td>\n",
       "      <td>the business development manager is a hunter t...</td>\n",
       "      <td>Texas, United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122123</th>\n",
       "      <td>Solugenix</td>\n",
       "      <td>Marketing Social Media Specialist</td>\n",
       "      <td>marketing social media specialist 70k 75ksan j...</td>\n",
       "      <td>San Juan Capistrano, CA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122124 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           company_name  \\\n",
       "0                 Corcoran Sawyer Smith   \n",
       "1                The National Exemplar    \n",
       "2                Abrams Fensterman, LLP   \n",
       "3             Downtown Raleigh Alliance   \n",
       "4                            Raw Cereal   \n",
       "...                                 ...   \n",
       "122119                     Lozano Smith   \n",
       "122120                        Pinterest   \n",
       "122121                     EPS Learning   \n",
       "122122  Trelleborg Applied Technologies   \n",
       "122123                        Solugenix   \n",
       "\n",
       "                                                    title  \\\n",
       "0                                   Marketing Coordinator   \n",
       "1                             Assitant Restaurant Manager   \n",
       "2       Senior Elder Law / Trusts and Estates Associat...   \n",
       "3                Economic Development and Planning Intern   \n",
       "4                                                Producer   \n",
       "...                                                   ...   \n",
       "122119                   Title IX/Investigations Attorney   \n",
       "122120       Staff Software Engineer, ML Serving Platform   \n",
       "122121               Account Executive, Oregon/Washington   \n",
       "122122                       Business Development Manager   \n",
       "122123                  Marketing Social Media Specialist   \n",
       "\n",
       "                                              description  \\\n",
       "0       job descriptiona leading real estate firm in n...   \n",
       "1       the national exemplar is accepting application...   \n",
       "2       senior associate attorney elder law trusts and...   \n",
       "3       job summarythe economic development planning i...   \n",
       "4       company descriptionraw cereal is a creative de...   \n",
       "...                                                   ...   \n",
       "122119  our walnut creek office is currently seeking a...   \n",
       "122120  about pinterest millions of people across the ...   \n",
       "122121  company overview eps learning is a leading k12...   \n",
       "122122  the business development manager is a hunter t...   \n",
       "122123  marketing social media specialist 70k 75ksan j...   \n",
       "\n",
       "                       location  \n",
       "0                 Princeton, NJ  \n",
       "1                Cincinnati, OH  \n",
       "2             New Hyde Park, NY  \n",
       "3                   Raleigh, NC  \n",
       "4                 United States  \n",
       "...                         ...  \n",
       "122119         Walnut Creek, CA  \n",
       "122120            United States  \n",
       "122121              Spokane, WA  \n",
       "122122     Texas, United States  \n",
       "122123  San Juan Capistrano, CA  \n",
       "\n",
       "[122124 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptions_df = pd.read_parquet(os.path.join(project_root, 'data', 'processed', 'cleaned_postings_modeling.parquet'))\n",
    "descriptions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f350763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text normalization function\n",
    "def normalize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    # Sentence tokenization\n",
    "    sentences = sent_tokenize(text)\n",
    "    # Word tokenization for each sentence\n",
    "    tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13f0598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from collections import Counter\n",
    "\n",
    "# Create corpus of tokenized descriptions\n",
    "corpus = []\n",
    "for desc in descriptions_df['description'].dropna():\n",
    "    tokenized_text = normalize_text(desc)\n",
    "    for sentence in tokenized_text:\n",
    "        corpus.append(sentence)\n",
    "\n",
    "# Set n-gram order (e.g., trigram model)\n",
    "n = 3\n",
    "\n",
    "# Prepare n-grams using NLTK's built-in function\n",
    "train_data, padded_vocab = padded_everygram_pipeline(n, corpus)\n",
    "\n",
    "# Create and train the model\n",
    "model = MLE(n)  # Maximum Likelihood Estimation\n",
    "model.fit(train_data, padded_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4390c1",
   "metadata": {},
   "source": [
    "### Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b2c55c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_job_description(model, num_words=100, text_seed=None):\n",
    "    if text_seed is None:\n",
    "        text_seed = ['we', 'are', 'looking', 'for']\n",
    "    else:\n",
    "        text_seed = word_tokenize(text_seed.lower())\n",
    "    \n",
    "    context = text_seed.copy()\n",
    "    output = context.copy()\n",
    "    \n",
    "    # Generate words\n",
    "    for _ in range(num_words):\n",
    "        # Get context for prediction (last n-1 words)\n",
    "        context = context[-(model.order-1):]\n",
    "        \n",
    "        # Generate next word\n",
    "        next_word = model.generate(1, context)\n",
    "        \n",
    "        # Add to output\n",
    "        output.append(next_word)\n",
    "        context.append(next_word)\n",
    "        \n",
    "        # Stop if we generate an end-of-sentence token\n",
    "        if next_word in ['.', '!', '?']:\n",
    "            break\n",
    "    \n",
    "    return ' '.join(output)\n",
    "\n",
    "def generate_with_sampling(model, num_words=100, text_seed=None, \n",
    "                          method='greedy', temp=1.0, k=10, p=0.9):\n",
    "    \"\"\"\n",
    "    Generate text using different sampling methods:\n",
    "    - 'greedy': Always choose the most likely next word\n",
    "    - 'random': Sample from the full probability distribution\n",
    "    - 'topk': Sample from the k most likely words\n",
    "    - 'nucleus': Sample from the top words that comprise p probability mass\n",
    "    - 'temperature': Apply temperature to soften/sharpen the distribution\n",
    "    \"\"\"\n",
    "    if text_seed is None:\n",
    "        text_seed = ['we', 'are', 'looking', 'for']\n",
    "    else:\n",
    "        text_seed = word_tokenize(text_seed.lower())\n",
    "    \n",
    "    context = text_seed.copy()\n",
    "    output = context.copy()\n",
    "    \n",
    "    for _ in range(num_words):\n",
    "        # Get context (last n-1 words)\n",
    "        context = context[-(model.order-1):]\n",
    "        \n",
    "        # Get the probability distribution for next words\n",
    "        dist = model.context_counts[tuple(context)]\n",
    "        \n",
    "        # Different sampling methods\n",
    "        if method == 'greedy':\n",
    "            # Get the most likely next word\n",
    "            next_word = max(dist.items(), key=lambda x: x[1])[0]\n",
    "            \n",
    "        elif method == 'random':\n",
    "            # Sample according to distribution\n",
    "            words, probs = zip(*dist.items())\n",
    "            total = sum(probs)\n",
    "            probs = [p/total for p in probs]  # Normalize to sum to 1\n",
    "            next_word = np.random.choice(words, p=probs)\n",
    "            \n",
    "        elif method == 'topk':\n",
    "            # Sample from top k most likely words\n",
    "            top_k = sorted(dist.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "            words, counts = zip(*top_k)\n",
    "            total = sum(counts)\n",
    "            probs = [c/total for c in counts]  # Normalize to sum to 1\n",
    "            next_word = np.random.choice(words, p=probs)\n",
    "            \n",
    "        elif method == 'nucleus':\n",
    "            # Nucleus (top-p) sampling\n",
    "            items = sorted(dist.items(), key=lambda x: x[1], reverse=True)\n",
    "            total = sum(item[1] for item in items)\n",
    "            cumulative = 0\n",
    "            nucleus = []\n",
    "            \n",
    "            for word, count in items:\n",
    "                nucleus.append((word, count))\n",
    "                cumulative += count/total\n",
    "                if cumulative >= p:\n",
    "                    break\n",
    "                    \n",
    "            words, counts = zip(*nucleus)\n",
    "            nucleus_total = sum(counts)\n",
    "            probs = [c/nucleus_total for c in counts]\n",
    "            next_word = np.random.choice(words, p=probs)\n",
    "            \n",
    "        elif method == 'temperature':\n",
    "            # Temperature sampling\n",
    "            words, counts = zip(*dist.items())\n",
    "            # Convert counts to log probabilities for numerical stability\n",
    "            logits = np.array([np.log(count) for count in counts])\n",
    "            # Apply temperature\n",
    "            logits = logits / temp\n",
    "            # Convert back to probabilities\n",
    "            probs = np.exp(logits)\n",
    "            probs = probs / np.sum(probs)  # Normalize\n",
    "            next_word = np.random.choice(words, p=probs)\n",
    "        \n",
    "        output.append(next_word)\n",
    "        context.append(next_word)\n",
    "        \n",
    "        if next_word in ['.', '!', '?']:\n",
    "            break\n",
    "    \n",
    "    return ' '.join(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa42662e",
   "metadata": {},
   "source": [
    "### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10489e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.lm.vocabulary import Vocabulary\n",
    "\n",
    "def calculate_perplexity(model, test_sentences):\n",
    "    \"\"\"Calculate perplexity of the model on test data\"\"\"\n",
    "    perplexities = []\n",
    "    for sentence in test_sentences:\n",
    "        try:\n",
    "            # Use n-grams from the test sentence\n",
    "            test_ngrams = list(ngrams(sentence, model.order))\n",
    "            if test_ngrams:\n",
    "                perplexity = model.perplexity(test_ngrams)\n",
    "                perplexities.append(perplexity)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    return np.mean(perplexities) if perplexities else float('inf')\n",
    "\n",
    "def evaluate_model(model, test_data, num_samples=5):\n",
    "    \"\"\"Evaluate the model using perplexity and BLEU score\"\"\"\n",
    "    # Calculate perplexity\n",
    "    perplexity = calculate_perplexity(model, test_data)\n",
    "    \n",
    "    # Generate samples and calculate BLEU scores\n",
    "    bleu_scores = []\n",
    "    samples = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        generated = generate_job_description(model, num_words=50)\n",
    "        samples.append(generated)\n",
    "        \n",
    "        # Calculate BLEU score against test data\n",
    "        references = [[sentence] for sentence in test_data]\n",
    "        hypothesis = word_tokenize(generated)\n",
    "        \n",
    "        smoothing = SmoothingFunction().method1\n",
    "        bleu = sentence_bleu(references, hypothesis, smoothing_function=smoothing)\n",
    "        bleu_scores.append(bleu)\n",
    "    \n",
    "    return {\n",
    "        \"perplexity\": perplexity,\n",
    "        \"avg_bleu\": np.mean(bleu_scores),\n",
    "        \"samples\": samples\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6042b889",
   "metadata": {},
   "source": [
    "### Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f752aa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(descriptions_df, test_size=0.2)\n",
    "\n",
    "# Train model on training data\n",
    "train_corpus = []\n",
    "for desc in train_df['description'].dropna():\n",
    "    tokenized_text = normalize_text(desc)\n",
    "    for sentence in tokenized_text:\n",
    "        train_corpus.append(sentence)\n",
    "\n",
    "train_data, padded_vocab = padded_everygram_pipeline(n, train_corpus)\n",
    "model.fit(train_data, padded_vocab)\n",
    "\n",
    "# Evaluate on test data\n",
    "test_corpus = []\n",
    "for desc in test_df['description'].dropna():\n",
    "    tokenized_text = normalize_text(desc)\n",
    "    for sentence in tokenized_text:\n",
    "        test_corpus.append(sentence)\n",
    "\n",
    "eval_results = evaluate_model(model, test_corpus)\n",
    "print(f\"Perplexity: {eval_results['perplexity']:.2f}\")\n",
    "print(f\"Average BLEU score: {eval_results['avg_bleu']:.4f}\")\n",
    "print(\"\\nSample generations:\")\n",
    "for i, sample in enumerate(eval_results['samples']):\n",
    "    print(f\"\\n[{i+1}] {sample}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b92a75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import Laplace, KneserNeyInterpolated\n",
    "\n",
    "# Try different smoothing methods\n",
    "laplace_model = Laplace(n)\n",
    "laplace_model.fit(train_data, padded_vocab)\n",
    "\n",
    "kn_model = KneserNeyInterpolated(n)\n",
    "kn_model.fit(train_data, padded_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956fcd1b",
   "metadata": {},
   "source": [
    "### Compare sampling methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e49ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_sampling_methods(model, test_data):\n",
    "    methods = ['greedy', 'random', 'topk', 'nucleus', 'temperature']\n",
    "    results = {}\n",
    "    \n",
    "    for method in methods:\n",
    "        print(f\"\\nEvaluating {method} sampling...\")\n",
    "        samples = []\n",
    "        \n",
    "        # Generate 3 examples with each method\n",
    "        for i in range(3):\n",
    "            if method == 'temperature':\n",
    "                # Try different temperatures\n",
    "                temps = [0.5, 1.0, 2.0]\n",
    "                sample = generate_with_sampling(model, num_words=50, \n",
    "                                              method=method, temp=temps[i])\n",
    "                samples.append(f\"Temperature {temps[i]}: {sample}\")\n",
    "            else:\n",
    "                sample = generate_with_sampling(model, num_words=50, method=method)\n",
    "                samples.append(sample)\n",
    "        \n",
    "        results[method] = samples\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the comparison\n",
    "sampling_results = compare_sampling_methods(model, test_corpus)\n",
    "\n",
    "# Print results\n",
    "for method, samples in sampling_results.items():\n",
    "    print(f\"\\n== {method.upper()} SAMPLING ==\")\n",
    "    for i, sample in enumerate(samples):\n",
    "        print(f\"[{i+1}] {sample}\")\n",
    "        print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-linkedin-offers-r2DELMkM-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
