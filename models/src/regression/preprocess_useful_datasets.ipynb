{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15cff9d4",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "Model purpose: given a job description, predict a score for the job description.\n",
    "\n",
    "### Download and loading of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11f05b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import pandas as pd\n",
    "# Download latest version to the specified directory\n",
    "# path = kagglehub.dataset_download(\"arshkon/linkedin-job-postings\")\n",
    "\n",
    "path = \"/home/leon/.cache/kagglehub/datasets/arshkon/linkedin-job-postings/versions/13\"\n",
    "\n",
    "print(f\"Path to dataset files: {path}\")\n",
    "print(f\"List of files in the dataset: {os.listdir(path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e00792",
   "metadata": {},
   "source": [
    "**Drop indexes with NaN values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "951bf8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "postings_path = path + \"/postings.csv\"\n",
    "postings_df = pd.read_csv(postings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93c84ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# print(postings_df['pat_period'].value_counts())\n",
    "# print(f\"Nb of YEARLY salary: {len(postings_df['pay_period'] == 'YEARLY')}\")\n",
    "# print(f\"Nb of HOURLY salary: {len(postings_df['pay_period'] == 'HOURLY')}\")\n",
    "\n",
    "\n",
    "max_sal_df = postings_df.copy()\n",
    "max_sal_df.dropna(subset=[\"normalized_salary\"], inplace=True)\n",
    "print(f\"Nb of single posting normalized salary: {max_sal_df['normalized_salary'].unique()}\")\n",
    "\n",
    "\n",
    "unique, counts = np.unique(max_sal_df[\"normalized_salary\"], return_counts=True)\n",
    "print(f\"Unique normalized salaries: {len(unique)}\")\n",
    "print(f\"Nb rows: {len(max_sal_df)}\")\n",
    "# Show job posting with max salary\n",
    "\n",
    "\n",
    "plt.stem(unique, counts)\n",
    "plt.xlim(0, 1000000)\n",
    "plt.xlabel(\"Normalized Salary\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylabel(\"Nb of Postings\")\n",
    "plt.title(\"Histogram of lViews\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "postings_df.sort_values(by='normalized_salary', ascending=False).head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38a0e0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of useful columns\n",
    "useful_cols = [\"job_id\", \"company_name\", \"title\", \"description\", \"views\", \"skills_desc\"]\n",
    "views_df = postings_df[useful_cols].copy()\n",
    "views_df.dropna(subset=[\"description\", \"views\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced326df",
   "metadata": {},
   "source": [
    "**Cleaning the descriptions of unwanted characters such as emojis etc**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f60cc378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Nettoyage de base\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'\\d{10,}', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply the clean_text function and ASSIGN the result back\n",
    "views_df['description'] = views_df['description'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06485719",
   "metadata": {},
   "source": [
    "### Data analysis (views column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ada05afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "\n",
    "# Quick overview of the dataset\n",
    "print(f\"Number of rows: {views_df.shape[0]}\")\n",
    "print(f\"Number of columns: {views_df.shape[1]}\")\n",
    "\n",
    "# display basic statistics on useful columns\n",
    "# for col in useful_cols:\n",
    "#     print(f\"{col}: {postings_df[col].dtype}\")\n",
    "#     print(f\"Statistics for {col}:\")\n",
    "#     print(postings_df[col].describe())\n",
    "#     print()\n",
    "\n",
    "# postings_df[\"views\"] = postings_df[\"views\"].astype(\"Int64\", errors=\"raise\")\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "# print(postings_df.head())\n",
    "# print(postings_df[\"description\"][0])\n",
    "\n",
    "# Dropna for the views and description columns\n",
    "# print(f\"Number of rows before dropping NaN values: {postings_df.shape[0]}\")\n",
    "# postings_df.dropna(subset=[\"views\", \"description\"], inplace=True)\n",
    "# print(f\"Number of rows after dropping NaN values: {postings_df.shape[0]}\")\n",
    "\n",
    "# Basic statistics on the views column\n",
    "print(views_df[\"views\"].describe())\n",
    "print(f\"Number of unique values in the 'views' column: {views_df[\"views\"].nunique()}\")\n",
    "print(\"Number of rows with 1 views:\", len(views_df[views_df[\"views\"] == 1]))\n",
    "print(\"Number of rows with 2 views:\", len(views_df[views_df[\"views\"] == 2]))\n",
    "print(\"Number of rows with 100 views:\", len(views_df[views_df[\"views\"] == 100]))\n",
    "\n",
    "test_views = np.log1p(views_df[\"views\"])\n",
    "unique, counts = np.unique(test_views, return_counts=True)\n",
    "\n",
    "plt.stem(unique, counts)\n",
    "plt.xlim(0, 10)\n",
    "plt.xlabel(\"Views\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "# plt.yscale(\"log\")\n",
    "plt.title(\"Stem plot for Views (log1p transformation)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "pt = PowerTransformer(method='box-cox')\n",
    "data_bc = pt.fit_transform(np.array(views_df[\"views\"]).reshape(-1, 1))\n",
    "\n",
    "unique, counts = np.unique(data_bc, return_counts=True)\n",
    "plt.stem(unique, counts)\n",
    "# plt.xlim(9000, 10000)\n",
    "plt.xlabel(\"Views\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "# plt.yscale(\"log\")\n",
    "plt.title(\"Stem plot of Views (Box-Cox transformation)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b2f4b1",
   "metadata": {},
   "source": [
    "### Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c779d4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "views_df[\"views\"] = views_df[\"views\"].astype(\"Int64\", errors=\"raise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09085d03",
   "metadata": {},
   "source": [
    "### Test Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b35b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, PoissonRegressor, GammaRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(views_df[\"description\"], views_df[\"views\"], test_size=0.2, random_state=42)\n",
    "\n",
    "pt = PowerTransformer(method='box-cox')\n",
    "\n",
    "y_train_bc = pt.fit_transform(np.array(y_train).reshape(-1, 1)).flatten()\n",
    "y_test_bc = pt.transform(np.array(y_test).reshape(-1, 1)).flatten()\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=2000, min_df=2, max_df=0.90, ngram_range=(1, 3), stop_words='english')\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "linear_reg_model = LinearRegression()\n",
    "linear_reg_model.fit(X_train_tfidf, y_train_bc)\n",
    "\n",
    "y_pred_bc = linear_reg_model.predict(X_test_tfidf)\n",
    "y_pred_original = pt.inverse_transform(y_pred_bc.reshape(-1, 1)).flatten()\n",
    "\n",
    "rmse = mean_squared_error(y_test, y_pred_original)\n",
    "print(f\"MSE: {rmse:.2f}\")\n",
    "\n",
    "# Optional: Check first few predictions vs actual\n",
    "results = pd.DataFrame({\n",
    "    \"Actual\": y_test,\n",
    "    \"Predicted\": y_pred_original\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3259a2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9de3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import PoissonRegressor, LinearRegression, Ridge\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Apply standard scaler to the views column\n",
    "# test_views = StandardScaler().fit_transform(views_df[\"views\"].values.reshape(-1, 1))\n",
    "\n",
    "filtered_views = np.where(views_df[\"views\"] > views_df[\"views\"].quantile(0.95),\n",
    "                views_df[\"views\"].quantile(0.95),\n",
    "                views_df[\"views\"])\n",
    "\n",
    "# Split data (X = text, y = views)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    views_df[\"description\"].to_numpy(),\n",
    "    filtered_views, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"dtype X_train:\", type(X_train))\n",
    "print(\"dtype views:\", type(filtered_views))\n",
    "\n",
    "# Define the text preprocessing pipeline\n",
    "text_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        max_features=5000,\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 3),\n",
    "        min_df=3,\n",
    "        max_df=0.8\n",
    "    )),\n",
    "])\n",
    "\n",
    "model_pipeline = Pipeline([\n",
    "    ('regressor', HistGradientBoostingRegressor(\n",
    "    max_iter=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=10\n",
    "))])\n",
    "\n",
    "# Full pipeline\n",
    "full_pipeline = Pipeline([\n",
    "    ('text_preprocessing', text_pipeline),\n",
    "    ('regressor', model_pipeline)\n",
    "])\n",
    "\n",
    "X_train_dense = text_pipeline.named_steps['tfidf'].fit_transform(X_train).toarray()\n",
    "X_test_dense = text_pipeline.named_steps['tfidf'].transform(X_test).toarray()\n",
    "\n",
    "# Train\n",
    "full_pipeline.fit(X_train_dense, y_train)\n",
    "\n",
    "# Predict (automatically applies inverse Box-Cox)\n",
    "y_pred = full_pipeline.predict(X_test_dense)\n",
    "\n",
    "# Round to integers (since views are counts)\n",
    "# y_pred = np.round(y_pred).astype(int)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error: {np.sqrt(mse):.2f}\")\n",
    "\n",
    "# Results\n",
    "results = pd.DataFrame({\"Actual\": y_test, \"Predicted\": y_pred})\n",
    "print(results.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43600d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import PoissonRegressor, LinearRegression, Ridge\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Apply standard scaler to the views column\n",
    "# test_views = StandardScaler().fit_transform(views_df[\"views\"].values.reshape(-1, 1))\n",
    "\n",
    "test = np.where(views_df[\"views\"] > views_df[\"views\"].quantile(0.99), views_df[\"views\"].quantile(0.99), views_df[\"views\"])\n",
    "print(test)\n",
    "# Split data (X = text, y = views)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    views_df[\"description\"],\n",
    "    test, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"dtype X_train:\", type(X_train.values))\n",
    "print(\"dtype views:\", type(views_df[\"views\"]))\n",
    "\n",
    "# Train (use toarray)\n",
    "full_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict (automatically applies inverse Box-Cox)\n",
    "y_pred = full_pipeline.predict(X_test)\n",
    "\n",
    "# Round to integers (since views are counts)\n",
    "# y_pred = np.round(y_pred).astype(int)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error: {np.sqrt(mse):.2f}\")\n",
    "\n",
    "# Results\n",
    "results = pd.DataFrame({\"Actual\": y_test, \"Predicted\": y_pred})\n",
    "print(results.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d65cb7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Results\n",
    "results = pd.DataFrame({\"Actual\": y_test, \"Predicted\": y_pred})\n",
    "print(results.head(50))\n",
    "\n",
    "# Plot the difference between actual and predicted values\n",
    "# Plot two curves: one for actual values and one for predicted values, use stem plots\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.stem(y_test.index, y_test, linefmt='b-', markerfmt='bo', basefmt=' ', label='Actual')\n",
    "# plt.stem(y_test.index, y_pred, linefmt='r-', markerfmt='ro', basefmt=' ', label='Predicted')\n",
    "# plt.xlabel('Index')\n",
    "# plt.xlim(0, 10000)\n",
    "# plt.ylabel('Views')\n",
    "# plt.yscale('log')\n",
    "# plt.title('Actual vs Predicted Views')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f47e531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "y_pred_int = np.round(y_pred_original).astype(int)\n",
    "# Optional: Check first few predictions vs actual\n",
    "results = pd.DataFrame({\n",
    "    \"Actual\": y_test,\n",
    "    \"Predicted\": y_pred_int\n",
    "})\n",
    "\n",
    "print(results.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed56530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other models tests\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, PoissonRegressor, GammaRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(views_df[\"description\"], views_df[\"views\"], test_size=0.2, random_state=42)\n",
    "\n",
    "pt = StandardScaler()\n",
    "\n",
    "# y_train_bc = pt.fit_transform(np.array(y_train).reshape(-1, 1)).flatten()\n",
    "# y_test_bc = pt.transform(np.array(y_test).reshape(-1, 1)).flatten()\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=5000, min_df=2, max_df=0.90, ngram_range=(1, 3), stop_words='english')\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "model = HistGradientBoostingRegressor(\n",
    "    loss=\"poisson\",  # Poisson loss for counts\n",
    "    max_iter=200,\n",
    "    learning_rate=0.1\n",
    ")\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "y_pred_int = np.round(y_pred).astype(int)\n",
    "\n",
    "\n",
    "rmse = mean_squared_error(y_test, y_pred_int)\n",
    "print(f\"MSE: {rmse:.2f}\")\n",
    "\n",
    "# Optional: Check first few predictions vs actual\n",
    "results = pd.DataFrame({\n",
    "    \"Actual\": y_test,\n",
    "    \"Predicted\": y_pred_int\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-linkedin-offers-4RXahsWL-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
